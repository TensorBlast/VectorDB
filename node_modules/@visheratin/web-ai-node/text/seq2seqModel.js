"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.Seq2SeqModel = void 0;
const common_js_1 = require("../common.js");
const base_js_1 = require("./base.js");
const prepare_js_1 = require("./prepare.js");
const index_js_1 = require("../index.js");
class Seq2SeqModel extends base_js_1.BaseTextModel {
    constructor(metadata) {
        super(metadata);
        this.init = async (proxy = true) => {
            const start = new Date();
            const encoderPath = this.metadata.modelPaths.get("encoder");
            if (!encoderPath) {
                throw new Error("model paths do not have the 'encoder' path");
            }
            const encoderOutputName = this.metadata.outputNames.get("encoder");
            if (!encoderOutputName) {
                throw new Error("output names do not have the 'encoder' path");
            }
            const encoderSession = await (0, index_js_1.createSession)(encoderPath, proxy);
            const decoderPath = this.metadata.modelPaths.get("decoder");
            if (!decoderPath) {
                throw new Error("model paths do not have the 'decoder' path");
            }
            const decoderOutputName = this.metadata.outputNames.get("decoder");
            if (!decoderOutputName) {
                throw new Error("output names do not have the 'decoder' path");
            }
            const decoderSession = await (0, index_js_1.createSession)(decoderPath, proxy);
            this.encoder = new common_js_1.Encoder(encoderSession, encoderOutputName, common_js_1.GeneratorType.Seq2Seq);
            this.decoder = new common_js_1.Decoder(decoderSession, decoderOutputName, common_js_1.GeneratorType.Seq2Seq);
            this.tokenizer = await (0, index_js_1.loadTokenizer)(this.metadata.tokenizerPath);
            const end = new Date();
            const elapsed = (end.getTime() - start.getTime()) / 1000;
            this.initialized = true;
            return elapsed;
        };
        this.process = async (inputs, prefix) => {
            if (!this.initialized ||
                !this.encoder ||
                !this.decoder ||
                !this.tokenizer) {
                throw Error("the model is not initialized");
            }
            if (typeof inputs === "string") {
                inputs = [inputs];
            }
            if (prefix && prefix.length > 0) {
                if (!this.metadata.prefixes) {
                    throw Error("the model does not support prefixes");
                }
                if (!this.metadata.prefixes.includes(prefix)) {
                    throw Error("the prefix is not allowed");
                }
                for (let i = 0; i < inputs.length; i++) {
                    inputs[i] = prefix + ": " + inputs[i];
                }
            }
            if (inputs.length == 1 && this.cache.has(inputs[0])) {
                return {
                    text: [this.cache.get(inputs[0])],
                    cached: true,
                    tokensNum: 0,
                    elapsed: 0,
                };
            }
            const textTensors = await (0, prepare_js_1.prepareTextTensors)(inputs, this.tokenizer, true, this.metadata.tokenizerParams.padTokenID);
            if (this.metadata.tokenizerParams.bosTokenID === undefined ||
                this.metadata.tokenizerParams.eosTokenID === undefined) {
                throw Error("the model does not have the bosTokenID or eosTokenID");
            }
            const generationConfig = {
                maxLength: 500,
                eosTokenID: this.metadata.tokenizerParams.eosTokenID,
                bosTokenID: this.metadata.tokenizerParams.bosTokenID,
                padTokenID: this.metadata.tokenizerParams.padTokenID,
            };
            const start = new Date();
            const outputTokenIDs = [];
            const result = [];
            const encoderOutput = await (0, common_js_1.encodeData)(undefined, undefined, undefined, this.encoder, textTensors[0], textTensors[1]);
            for await (const tokenIDs of (0, common_js_1.generate)(encoderOutput, this.decoder, generationConfig, textTensors[1])) {
                for (let i = 0; i < tokenIDs.length; i++) {
                    if (tokenIDs[i] !== this.metadata.tokenizerParams.padTokenID) {
                        if (!outputTokenIDs[i])
                            outputTokenIDs[i] = [];
                        outputTokenIDs[i].push(tokenIDs[i]);
                    }
                    const outputTokens = new Uint32Array(outputTokenIDs[i]);
                    const res = await this.tokenizer.decode(outputTokens, true);
                    const output = res.trim();
                    result[i] = output;
                }
            }
            const end = new Date();
            const elapsed = (end.getTime() - start.getTime()) / 1000;
            return {
                text: result,
                cached: false,
                tokensNum: textTensors[0].data.length,
                elapsed: elapsed,
            };
        };
        this.cache = new Map();
    }
    async *processStream(inputs, prefix) {
        if (!this.initialized ||
            !this.encoder ||
            !this.decoder ||
            !this.tokenizer) {
            throw Error("the model is not initialized");
        }
        if (typeof inputs === "string") {
            inputs = [inputs];
        }
        if (prefix && prefix.length > 0) {
            if (!this.metadata.prefixes) {
                throw Error("the model does not support prefixes");
            }
            if (!this.metadata.prefixes.includes(prefix)) {
                throw Error("the prefix is not allowed");
            }
            for (let i = 0; i < inputs.length; i++) {
                inputs[i] = prefix + ": " + inputs[i];
            }
        }
        if (inputs.length == 1 && this.cache.has(inputs[0])) {
            return this.cache.get(inputs[0]);
        }
        const textTensors = await (0, prepare_js_1.prepareTextTensors)(inputs, this.tokenizer, true, this.metadata.tokenizerParams.padTokenID);
        if (this.metadata.tokenizerParams.bosTokenID === undefined ||
            this.metadata.tokenizerParams.eosTokenID === undefined) {
            throw Error("the model does not have the bosTokenID or eosTokenID");
        }
        const generationConfig = {
            maxLength: 500,
            eosTokenID: this.metadata.tokenizerParams.eosTokenID,
            bosTokenID: this.metadata.tokenizerParams.bosTokenID,
            padTokenID: this.metadata.tokenizerParams.padTokenID,
        };
        const outputTokenIDs = [];
        let oldOutput = new Array(inputs.length).fill("");
        const diffs = new Array(inputs.length).fill("");
        const encoderOutput = await (0, common_js_1.encodeData)(undefined, undefined, undefined, this.encoder, textTensors[0], textTensors[1]);
        for await (const tokenIDs of (0, common_js_1.generate)(encoderOutput, this.decoder, generationConfig, textTensors[1])) {
            const newOutput = [];
            for (let i = 0; i < tokenIDs.length; i++) {
                if (tokenIDs[i] !== this.metadata.tokenizerParams.padTokenID) {
                    if (!outputTokenIDs[i])
                        outputTokenIDs[i] = [];
                    outputTokenIDs[i].push(tokenIDs[i]);
                }
                const outputTokens = new Uint32Array(outputTokenIDs[i]);
                const output = await this.tokenizer.decode(outputTokens, true);
                newOutput.push(output);
                diffs[i] = output.substring(oldOutput[i].length);
            }
            yield diffs;
            oldOutput = newOutput;
        }
    }
}
exports.Seq2SeqModel = Seq2SeqModel;
//# sourceMappingURL=seq2seqModel.js.map